## Prompt Instruction
Please summarize the following parts of this transcript:

## Chapters Included
1:55:22 - Boston Dynamics AI Institute, 1:56:53 - Fear of robots, 2:07:16 - Running a company, 2:12:52 - Consciousness, 2:20:26 - Advice for young people, 2:22:21 - Future of robots

## Transcript
[01:53:53] you're trying to ask a
theoretical question in ChatGPT, [01:53:58] it could be true, or it may not be true. [01:54:00] And it's hard to have that verifier, [01:54:03] what is that truth (laughs)
that you're comparing against, [01:54:05] whereas, in physical
reality, you know the truth. [01:54:08] And this is an important difference. [01:54:10] And so, I think there is reason
to be a little bit concerned [01:54:15] about, you know, how these tools, [01:54:19] large language models could be used. [01:54:21] But I'm not very worried about
how they're gonna be used, [01:54:25] well, how learning algorithms [01:54:27] in general are going
to be used on robotics. [01:54:30] It's really a different
application that has different ways [01:54:35] of verifying what's going on. [01:54:36] - Well, the nice thing
about language models is [01:54:38] that I ultimately see, I'm really excited [01:54:42] about the possibility of
having conversations with Spot. [01:54:44] - [Robert] Yeah. [01:54:45] - There's no, I would say,
negative consequences to that [01:54:48] but just increasing the
bandwidth and the variety [01:54:51] of ways you can communicate
with this particular robot. [01:54:54] - [Robert] Yeah. [01:54:55] - So, you could communicate visually. [01:54:56] You can communicate through
some interface and to be able [01:55:00] to communicate verbally
again with a beer and so on. [01:55:03] I think that's really exciting to make [01:55:05] that much, much easier. [01:55:07] - We have this partner
Levatas that's adding [01:55:10] the vision algorithms
for gauge reading for us. [01:55:14] Just this week I saw a demo
where they hooked up, you know, [01:55:18] a language tool to Spot,
and they're talking to Spot [01:55:20] to give commands.
- Nice, I love it. [01:55:22] - Yeah. [01:55:22] - Can you tell me about the
Boston Dynamics AI Institute? [01:55:25] What is it, and what is its mission? [01:55:28] - So, it's a separate organization, [01:55:31] the Boston Dynamics Artificial
Intelligence Institute. [01:55:34] It's led by Marc Raibert, the
founder of Boston Dynamics, [01:55:37] and the former CEO, and
my old advisor at MIT. [01:55:41] Marc has always loved the research, [01:55:45] the pure research without the confinement [01:55:48] or demands of commercialization. [01:55:51] And he wanted to continue [01:55:55] to, you know, pursue that
unadulterated research [01:56:00] and so suggested to Hyundai [01:56:05] that he set up this institute, [01:56:07] and they agree that it's
worth additional investment [01:56:10] to kinda continue pushing this forefront. [01:56:14] And we expect to be working together [01:56:17] where you know Boston Dynamics is, again, [01:56:19] both commercialize and do research, [01:56:21] but the sort of time horizon [01:56:24] of the research we're
gonna do is, you know, [01:56:25] in the next, let's say
five years, you know. [01:56:28] What can we do in the next five years? [01:56:29] Let's work on those problems. [01:56:31] And I think the goal
of the AI Institute is [01:56:34] to work even further out. [01:56:36] Certainly, you know, the analogy
of legged locomotion again, [01:56:40] when we started that, that
was a multi-decade problem. [01:56:43] And so, I think Marc
wants to have the freedom [01:56:46] to pursue really hard
over-the-horizon problems. [01:56:51] That'll be the goal of the institute. [01:56:53] - So, we mentioned some of the
dangers, some of the concerns [01:56:58] about large language models. [01:57:00] That said, you know, there's
been a long-running fear [01:57:04] of these embodied robots. [01:57:07] Why do you think people are afraid [01:57:09] (Robert laughs) [01:57:10] of legged robots? [01:57:11] - Yeah, I wanted to show you this. [01:57:14] So, this is in the Wall Street Journal, [01:57:16] and this is all about ChatGPT, right? [01:57:18] But look at the picture. [01:57:20] - Yeah. [01:57:21] - [Robert] It's a humanoid robot. [01:57:22] - That's saying, "I will replace you." [01:57:24] - That looks scary, and it says, [01:57:25] "I'm gonna replace you." [01:57:27] - [Lex] Yeah. [01:57:27] - And so, the humanoid robot is [01:57:30] the embodiment of this ChatGPT tool [01:57:34] that there's reason to
be a little bit nervous [01:57:38] about how it gets deployed. [01:57:39] - [Lex] Yeah. [01:57:40] - So, I'm nervous about that connection. [01:57:44] It's unfortunate that
they chose to use a robot [01:57:46] as that embodiment. [01:57:48] As you and I just said, there's
big differences in this. [01:57:53] But people are afraid because
we've been taught to be afraid [01:57:59] for over 100 years. [01:58:01] So, you know, the word robot was developed [01:58:03] by a playwright named Karel Capek in 1921, [01:58:06] a Czech playwright,
"Rossum's Universal Robots." [01:58:10] And in that first depiction of a robot, [01:58:13] the robots took over (laughs)
at the end of the story. [01:58:17] And you know, people love to be afraid. [01:58:19] And so, we've been
entertained by these stories [01:58:22] for 100 years, and I think that's as much [01:58:26] why people are afraid as anything else, [01:58:28] as we've been sort of taught that this is [01:58:31] the logical progression through fiction. [01:58:37] I think it's fiction. [01:58:38] - I think what people more
and more will realize, [01:58:42] just like you said, that the threat, [01:58:46] like say you have a
super-intelligent AI embodied [01:58:49] in a robot. [01:58:50] That's much less threatening
because it's visible. [01:58:53] It's verifiable. [01:58:55] It's right there in physical reality. [01:58:57] And we humans know how to
deal with physical reality. [01:59:00] I think it's much scarier when
you have arbitrary scaling [01:59:04] of intelligent AI systems
in the digital space [01:59:08] that they could pretend to be human. [01:59:12] So, robot Spot is not gonna pretend. [01:59:14] It could pretend it's human all it wants. [01:59:16] (Lex laughs) [01:59:17] You could put ChatGPT on top of it, [01:59:19] but you're gonna know it's not human [01:59:21] because you have a contact
with physical reality. [01:59:23] - And you're gonna know
whether or not it's doing [01:59:25] what you asked it to do. [01:59:25] - Yeah, like, it's not gonna, (laughs) [01:59:29] I mean, I'm sure it can start,
just like a dog lies to you. [01:59:32] It's like, "I wasn't part
of tearing up that couch." [01:59:35] So, Spot can try
(Robert laughs) [01:59:37] to lie that like, you know, [01:59:38] "It wasn't me that's spilled that thing," [01:59:40] but you're going to kind
of figure it out eventually [01:59:44] if it happens multiple times, you know. [01:59:47] But I think that- [01:59:49] - Humanity has figured out
how to make machines safe. [01:59:52] - [Lex] Yeah. [01:59:52] - And there's, you know,
the regulatory environments [01:59:56] and certification protocols
that we've developed in order [02:00:00] to figure out how to make machines safe. [02:00:03] We don't know and don't have
that experience with software [02:00:08] that can be propagated
worldwide in an instant. [02:00:12] And so, I think we needed
to develop those protocols [02:00:14] and those tools, and so
that's work to be done. [02:00:19] But I don't think the fear of that [02:00:21] and that work should
necessarily impede our ability [02:00:24] to now get robots out because again, [02:00:27] I think we can judge when
a robot's being safe. [02:00:29] - So, and again, just like in that image, [02:00:32] there's a fear that
robots will take our jobs. [02:00:37] I took a ride. [02:00:38] I was in San Francisco. [02:00:38] I took a ride in a Waymo vehicle. [02:00:39] It's an autonomous vehicle, [02:00:41] and I've done it several times. [02:00:44] They're doing incredible work over there, [02:00:47] but (laughs) people flicked it off. [02:00:50] - Oh, really?
- Flicked off the car. [02:00:52] So, (laughs) I mean, that's a long story [02:00:55] of what the psychology of that is. [02:00:57] It could be maybe big tech,
or I don't know exactly [02:01:00] what they're flicking off. [02:01:02] - [Robert] Yeah. [02:01:03] - But there is an element of, like, [02:01:04] "These robots are taking our jobs," [02:01:06] or irreversibly transforming society such [02:01:10] that it will have economic impact, [02:01:11] and the little guy would lose a lot, [02:01:15] would lose their well-being. [02:01:16] Is there something to
be said about the fear [02:01:21] that robots will take our jobs? [02:01:23] - You know, [02:01:24] at every significant
technological transformation, [02:01:30] there's been fear of, you
know, an automation anxiety- [02:01:33] - Yes.
- that it's gonna have [02:01:35] a broader impact than we expected. [02:01:40] And there will be, you
know, jobs will change. [02:01:46] Sometime in the future, we're
gonna look back at people [02:01:49] who manually unloaded
these boxes from trailers, [02:01:52] and we're gonna say, "Why did
we ever do that manually?" [02:01:54] But there's a lotta people
who are doing that job today [02:01:57] that could be impacted. [02:02:01] But I think the reality
is, as I said before, [02:02:03] we're gonna build the technology [02:02:05] so that those very same
people can operate it. [02:02:07] And so, I think there's a pathway [02:02:09] to upskilling and operating. [02:02:11] Just like, look, we used
to farm with hand tools, [02:02:13] and now we farm with machines, [02:02:15] and nobody has really
regretted that transformation. [02:02:20] And I think the same can be
said for a lot of manual labor [02:02:22] that we're doing today. [02:02:24] And on top of that, you know, look, [02:02:27] we're entering a new world where
demographics are gonna have [02:02:32] strong impact on economic growth, [02:02:35] and you know, the advanced, [02:02:38] the first world is losing
population quickly. [02:02:43] In Europe, they're worried
about hiring enough people [02:02:47] just to keep the logistics
supply chain going. [02:02:51] And you know, part of this
is the response to COVID, [02:02:55] and everybody's sort of thinking back [02:02:58] what they really wanna do with their life, [02:03:00] but these jobs are getting
harder and harder to fill. [02:03:03] And I'm hearing that over and over again. [02:03:06] So, I think, frankly, this
is the right technology [02:03:08] at the right time where we're
gonna need some of this work [02:03:14] to be done, and we're gonna want tools [02:03:16] to enhance that productivity. [02:03:18] - And the scary impact, I think, again, [02:03:22] GPT comes to the rescue in terms [02:03:24] of being much more terrifying. [02:03:26] (Robert laughs) [02:03:27] (Lex laughs) [02:03:28] The scary impact of, basically, [02:03:30] so I'm, I guess, a software
person, so I program a lot. [02:03:33] And the fact that people like
me could be easily replaced [02:03:38] by GPT, that's going to have a- [02:03:43] - Well, and lot, you know,
anyone who deals with texts [02:03:46] and writing a draft proposal
might be easily done [02:03:50] with ChatGPT now. [02:03:52] - Yeah.
- where- [02:03:53] - Consultants.
- it wasn't before. [02:03:54] - [Lex] Journalists. [02:03:56] - Yeah. [02:03:57] - [Lex] Everybody is sweating. [02:03:58] - But on the other hand, you
also want it to be right. [02:04:01] And they don't know how
to make it right yet. [02:04:05] But it might make a good starting
point for you to iterate. [02:04:07] - Boy, do I have to talk to
you about modern journalism. [02:04:10] (Robert laughs) [02:04:11] That's another conversation altogether, [02:04:14] but yes, more right than the average, [02:04:21] the mean journalist, yes. [02:04:25] You spearheaded the weaponization letter [02:04:29] Boston Dynamics has. [02:04:30] Can you describe what that letter states [02:04:34] and the general topic of
the use of robots in war? [02:04:41] - We authored a letter and then got [02:04:45] several leading robotics
companies around the world, [02:04:49] including, you know, Unitree in China, [02:04:52] and Agility here in the United States, [02:04:57] and ANYmal in Europe, [02:05:01] and, you know, some others
to cosign a letter that said [02:05:05] we won't put weapons on our robots. [02:05:09] And part of the motivation
there is, you know, [02:05:11] as these robots start to
become commercially available, [02:05:16] you can see videos online of
people who've gotten a robot, [02:05:19] and strapped a gun on it, and
shown that they can, you know, [02:05:22] operate the gun remotely while
driving the robot around. [02:05:26] And so, having a robot that
has this level of mobility [02:05:30] and that can easily be configured in a way [02:05:33] that could harm somebody
from a remote operator is [02:05:36] justifiably a scary thing. [02:05:39] And so, we felt like it was important [02:05:41] to draw a bright line there and say, [02:05:44] "We're not going to allow this," [02:05:46] for, you know, reasons that we
think ultimately it's better [02:05:51] for the whole industry
if it grows in a way [02:05:55] where robots are ultimately
going to help us all [02:05:59] and make our lives more
fulfilled and productive. [02:06:03] But by goodness, you're gonna
have to trust the technology, [02:06:07] to let it in. [02:06:09] And if you think the
robot's gonna harm you, [02:06:13] that's gonna impede the
growth of that industry. [02:06:16] So, we thought it was
important to draw a bright line [02:06:22] and then publicize that. [02:06:24] And our plan is to, you
know, begin to engage [02:06:28] with lawmakers and regulators. [02:06:31] Let's figure out what
the rules are going to be [02:06:34] around the use of this
technology and use our position [02:06:40] as leaders in this industry and technology [02:06:44] to help force that issue. [02:06:48] In fact, I have a policy,
you know, director [02:06:51] at my company whose job it
is to engage with the public, [02:06:55] to engage with interested
parties, including regulators, [02:07:00] to sort of begin these discussions. [02:07:03] - Yeah, it's a really important topic, [02:07:04] and it's an important
topic for people that worry [02:07:07] about the impact of robots on our society [02:07:09] with autonomous weapon systems. [02:07:12] So, I'm glad you're sort
of leading the way in this. [02:07:16] You are the CEO of Boston Dynamics. [02:07:19] What's it take to be a
CEO of a robotics company? [02:07:21] So, you started as a
humble engineer, (laughs) [02:07:26] a PhD. [02:07:29] Just looking at your journey, [02:07:32] what does it take to go
from building the thing [02:07:37] to leading a company? [02:07:40] What are some of the
big challenges for you? [02:07:44] - Courage I would put front and
center for multiple reasons. [02:07:50] I talked earlier about the
courage to tackle hard problems. [02:07:54] So, I think there's courage
required not just of me [02:07:56] but of all of the people
who work at Boston Dynamics. [02:08:01] I also think we have a lot
of really smart people. [02:08:03] We have people who are
way smarter than I am. [02:08:06] And it takes a kinda courage
to be willing to lead them [02:08:11] and to trust that you have
something to offer to somebody [02:08:15] who probably is maybe a
better engineer than I am. [02:08:23] Adaptability, you know, it's
been a great career for me. [02:08:27] I never would've guessed I'd stayed [02:08:29] in one place for 30 years, and
the job has always changed. [02:08:36] I didn't really aspire to be
CEO from the very beginning, [02:08:40] but it was the natural
progression of things. [02:08:44] There always needed to be some level [02:08:46] of management that was needed. [02:08:48] And so, you know, when I saw something [02:08:52] that needed to be done
that wasn't being done, [02:08:54] I just stepped in to go do it. [02:08:55] And oftentimes because we were full [02:08:58] of such strong engineers,
oftentimes that was [02:09:03] in the management direction, [02:09:04] or it was in the business
development direction [02:09:08] or organizational, hiring. [02:09:10] Geez, I was the main person
hiring at Boston Dynamics [02:09:13] for probably 20 years, so I
was the head of HR, basically. [02:09:18] You know, just willingness
to sort of tackle any piece [02:09:21] of the business that needs
it and be willing to shift. [02:09:26] - Is there something you
could say to what it takes [02:09:28] to hire a great team? [02:09:31] What's a good interview process? [02:09:33] How do you know the guy or gal
are gonna make a great member [02:09:39] of a engineering team that's doing some [02:09:42] of the hardest work in the world? [02:09:45] - You know, we developed
an interview process [02:09:47] that I was quite fond of. [02:09:50] It's a little bit of a
hard interview process [02:09:52] because the best
interviews, you ask somebody [02:09:56] about what they're interested
in and what they're good at, [02:10:00] and if they can describe to you something [02:10:03] that they worked on, and you
saw they really did the work, [02:10:06] they solved the problems, and
you saw their passion for it, [02:10:13] but what makes that hard is you have [02:10:15] to ask a probing question about it. [02:10:16] You have to be smart enough
about what they're telling you [02:10:20] they're expert at to ask a good question. [02:10:23] And so, it takes a pretty
talented team to do that. [02:10:28] But if you can do that,
that's how you tap into, [02:10:30] "Ah, this person cares about their work. [02:10:33] They really did the work. [02:10:34] They're excited about it." [02:10:35] That's the kind of person
I want at my company. [02:10:39] You know, at Google, they taught us [02:10:41] about their interview process, [02:10:43] and it was a little bit different. [02:10:46] You know, we evolved the
process at Boston Dynamics [02:10:51] where it didn't matter
if you were an engineer, [02:10:54] or you were an administrative assistant, [02:10:57] or a financial person, or a technician. [02:11:00] You gave us a presentation. [02:11:01] You came in, and you
gave us a presentation. [02:11:03] You had to stand up and
talk in front of us. [02:11:06] And I just thought that was
great to tap into those things [02:11:09] I just described to you. [02:11:10] At Google, they taught us,
and I understand why, right. [02:11:14] They're hiring tens of
thousands of people. [02:11:17] They need a more standardized process. [02:11:19] So, they would sort of
err on the other side [02:11:21] where they would ask
you a standard question. [02:11:23] I'm gonna ask you a programming question, [02:11:25] and I'm just gonna ask you [02:11:26] to, you know, write code in front of me. [02:11:28] That's a terrifying, you
know, application process. [02:11:32] - [Lex] Yeah. [02:11:33] - It does let you compare
candidates really well, [02:11:37] but it doesn't necessarily
let you tap into who they are. [02:11:40] - Yeah.
- (laughs) Right? [02:11:41] 'Cause you're asking them
to answer your question [02:11:43] instead of you asking them about
what they're interested in. [02:11:47] But frankly, that
process is hard to scale. [02:11:50] And even at Boston Dynamics, [02:11:52] we're not doing that
with everybody anymore. [02:11:55] But we are still doing that [02:11:56] with, you know, the technical people [02:12:00] because we too now need to sort [02:12:02] of increase our rate of hiring. [02:12:04] Not everybody's giving
a presentation anymore. [02:12:06] - But you're still
ultimately trying to find [02:12:08] that basic seed of passion- [02:12:11] - Yeah, and talent.
- for the world. [02:12:12] - You know, did they really do it? [02:12:15] Did they find something
interesting or curious, you know, [02:12:19] and do they care about it? (laughs) [02:12:20] - I think somebody I admire is Jim Keller, [02:12:25] and he likes details. [02:12:30] So, one of the ways you could, (laughs) [02:12:32] if you get a person to talk
about what they're interested [02:12:35] in, how many details, like, how much [02:12:37] of the whiteboard can you fill out? [02:12:39] - Yeah.
- What they- [02:12:40] - Well, I think you figure out
did they really do the work [02:12:42] if they know some of the details. [02:12:43] - Yes.
- And if they have [02:12:44] to wash over the details,
well, then they didn't do it. [02:12:46] - They didn't do it.
(Robert laughs) [02:12:47] 'Cause especially with engineering, [02:12:49] the work is in the details. [02:12:50] - Yeah. [02:12:52] - I have to go there briefly (sighs) [02:12:56] just to get your kind of thoughts [02:12:58] on the long-term future of robotics. [02:13:02] There's been discussions on the GPT side, [02:13:04] on the large language model
side of whether there's [02:13:07] consciousness inside
these language models. [02:13:12] And I think there's
fear, but I think there's [02:13:15] also excitement or at least the wide world [02:13:19] of opportunity and
possibility in embodied robots [02:13:23] having something like,
let's start with emotion, [02:13:28] love towards other human beings [02:13:31] and perhaps the display, real
or fake, of consciousness. [02:13:37] Is this something you think about in terms [02:13:40] of long-term future? [02:13:42] Because, as we've talked about, [02:13:45] people do anthropomorphize these robots. [02:13:49] It's difficult not to project some level [02:13:52] of, I use the word sentience, [02:13:55] some level of sovereignty, identity, [02:13:59] all the things we think as human. [02:14:00] That's what anthropomorphization
is, is we project humanness [02:14:04] onto mobile, especially legged robots. [02:14:09] Is that something almost [02:14:10] from a science-fiction
perspective you think about? [02:14:12] Or, do you try to avoid ever, [02:14:17] try to avoid the topic of
consciousness altogether? [02:14:21] - I'm certainly not an expert in it, [02:14:22] (Lex laughs)
and I don't spend- [02:14:23] - Is anybody?
- a lot of time thinking [02:14:24] about this, right? [02:14:26] And I do think it's fairly
remote for the machines [02:14:29] that we're dealing with. [02:14:34] You're right that people anthropomorphize. [02:14:36] They read into the robots'
intelligence and emotion [02:14:40] that isn't there because
they see physical gestures [02:14:45] that are similar to
things they might even see [02:14:47] in people or animals. [02:14:50] I don't know much [02:14:51] about how these large
language models really work. [02:14:54] I believe it's a kind
of statistical averaging [02:14:58] of the most common responses, you know, [02:15:00] to a series of words, right? [02:15:01] It's sort of a very
elaborate word completion. [02:15:10] And I'm dubious that that has [02:15:13] anything to do with consciousness. [02:15:17] And I even wonder if that model of sort [02:15:20] of simulating consciousness
by stringing words together [02:15:23] that are statistically
associated with one another, [02:15:28] whether or not that kind of knowledge, [02:15:30] if you wanna call that knowledge, [02:15:33] would be the kind of knowledge
that allowed a sentient being [02:15:38] to grow or evolve. [02:15:40] It feels to me like there's
something about truth [02:15:45] or emotions that's just a very
different kind of knowledge [02:15:50] that is absolute. [02:15:52] The interesting thing about
truth is it's absolute, [02:15:54] and it doesn't matter how
frequently it's represented [02:15:56] in the worldwide web. [02:15:59] If you know it to be
true, it it can only be, [02:16:01] it may only be there once,
but by God, that's true. [02:16:04] And I think emotions are a
little bit like that, too. [02:16:06] You know something, you know, [02:16:11] and I just think that's a
different kind of knowledge [02:16:13] than the way these large
language models derive sort [02:16:18] of simulated- [02:16:19] - It does seem that-
- intelligence. [02:16:21] - things that are true very well might be [02:16:25] statistically well
represented on the Internet [02:16:29] because the Internet's made up of humans. [02:16:32] So, I tend to suspect that
large language models are going [02:16:36] to be able to simulate
consciousness very effectively. [02:16:39] And I actually believe that current GPT 4, [02:16:42] when fine-tuned correctly,
would be able to do just that. [02:16:46] And there's going to be a lot [02:16:47] of very complicated
ethical questions that have [02:16:49] to be dealt with that have
nothing to do with robotics [02:16:53] and everything to do with- [02:16:55] - There needs to be some
process of labeling, [02:16:59] I think, (laughs) what is true [02:17:03] because there is also
disinformation available on the web, [02:17:07] and these models are going
to consider that kind [02:17:10] of information as well. [02:17:12] And again, you can't average
something that's true [02:17:15] and something that's
untrue and get something [02:17:17] that's moderately true. (laughs) [02:17:19] It's either right, or it's wrong. [02:17:22] And so, how is that process,
and this is obviously something [02:17:26] that the purveyors of
these, Bard and ChatGPT, [02:17:30] I'm sure this is what they're working on. [02:17:31] - Well, if you interact on
some controversial topics [02:17:34] with these models, they're
actually refreshingly nuanced. [02:17:41] Well, you realize there's
no one truth, you know. [02:17:46] What caused the war in Ukraine, right? [02:17:51] Any geopolitical conflict, you
can ask any kind of question, [02:17:54] especially the ones that
are politically tense, [02:17:59] divisive, and so on. [02:18:00] GPT is very good at presenting, [02:18:04] it presents the different hypotheses. [02:18:06] It presents calmly
(laughing) sort of the amount [02:18:10] of evidence for each one. [02:18:13] It's really refreshing. [02:18:14] It makes you realize
that truth is nuanced, [02:18:17] and it does that well. [02:18:18] And I think, with consciousness, [02:18:21] it would very accurately say, [02:18:25] "Well, it sure as hell feels
like I'm one of you humans, [02:18:30] but where's my body? [02:18:31] (Robert laughs) [02:18:32] I don't understand." [02:18:33] Like, you're going to be confused. [02:18:35] The cool thing about GPT is
it seems to be easily confused [02:18:40] in the way we are. [02:18:41] Like, you wake up in a new
room, and you ask, "Where am I?" [02:18:45] It seems to be able to
do that extremely well. [02:18:49] It'll tell you one thing, like a fact [02:18:51] about when a war started,
and when you correct it, say, [02:18:53] "Well, that's not consistent," [02:18:55] it'll be confused. [02:18:56] It'll be, "Yeah, you're right." [02:18:58] It'll have that same
element, childlike element [02:19:02] with humility of trying to
figure out its way in the world. [02:19:07] And I think that's a really tricky area [02:19:11] to sort of figure out with
us humans of what we want [02:19:15] to allow AI systems to say to us. [02:19:18] Because then, if there's
elements of sentience [02:19:21] that are on display, you can then start [02:19:24] to manipulate human emotion,
all that kinda stuff. [02:19:27] But I think that's a really serious [02:19:30] and aggressive discussion
that needs to be had [02:19:33] (laughing) on the software side. [02:19:35] I think, again, embodiment,
robotics are actually saving us [02:19:41] from the arbitrary scaling
of software systems [02:19:45] versus creating more problems. [02:19:48] But that said, I really believe [02:19:50] in that connection
between human and robot. [02:19:52] There's magic there. [02:19:54] And I think there's also, I think, [02:19:57] a lot of money to be made there. [02:19:59] And Boston Dynamics is leading the world [02:20:01] in the most elegant movement [02:20:07] done by robots. [02:20:08] (Robert laughs) [02:20:09] So, I can't wait-
- Well, thank you. [02:20:11] - to what maybe other
people that built on top [02:20:14] of Boston Dynamics robots or
Boston Dynamics by itself. [02:20:20] So, you had one wild career, [02:20:23] one place on one set of problems
but incredibly successful. [02:20:29] Can you give advice to young
folks today in high school, [02:20:33] maybe in college looking
out into this future [02:20:36] where so much robotics [02:20:40] and AI seems to be defining [02:20:43] the trajectory of human civilization. [02:20:44] Can you give 'em advice
on how to have a career [02:20:48] they can be proud of or how to have [02:20:50] a life they can be proud of? [02:20:53] - Well, I would say, you
know, follow your heart [02:20:57] and your interest. [02:20:59] Again, this was an organizing
principle, I think, [02:21:01] behind the Leg Lab at MIT that turned [02:21:06] into a value at Boston Dynamics, [02:21:09] which was follow your curiosity. [02:21:13] Love what you're doing. [02:21:16] You'll have a lot more fun,
and you'll be a lot better [02:21:18] at it as a result. [02:21:24] I think it's hard to plan, you know. [02:21:26] Don't get too hung up on
planning too far ahead. [02:21:30] Find things that you like doing [02:21:32] and then see where it takes you. [02:21:33] You can always change direction. [02:21:35] You will find things that, you know, [02:21:36] "Ah, that wasn't a good move. [02:21:38] I'm gonna pack up and
go do something else." [02:21:40] So, when people are
trying to plan a career, [02:21:43] I always feel like, "Ah,
there's a few happy mistakes [02:21:46] that happen along the way
and just live with that it." [02:21:49] You know, but make choices then. [02:21:52] So, avail yourselves to these
interesting opportunities, [02:21:55] like when I happened to run
into Marc down in the lab, [02:21:57] the basement of the AI lab, but be willing [02:22:00] to make a decision and then pivot [02:22:03] if you see something
exciting to go at, you know, [02:22:05] 'cause, if you're out and about enough, [02:22:09] you'll find things like
that that get you excited. [02:22:12] - So, there was a feeling
when you first met Marc [02:22:14] and saw the robots that
there's something interesting. [02:22:16] - "Oh, boy, I gotta go do this." [02:22:19] There was no doubt.
(Lex laughs) [02:22:20] (Robert laughs) [02:22:21] - What do you think in 100 years, whoo, [02:22:25] what do you think Boston
Dynamics is doing? [02:22:29] Even bigger, what do you think is the role [02:22:31] of robots in society? [02:22:32] Do you think we'll be seeing
billions of robots everywhere? [02:22:39] Do you think about that long-term vision? [02:22:42] - Well, I do think that, [02:22:47] I think that robots will be ubiquitous, [02:22:49] and they will be out amongst us, [02:22:55] and they'll be certainly doing, you know, [02:22:59] some of the hard labor that we do today. [02:23:03] I don't think people don't wanna work. [02:23:05] People wanna work. [02:23:06] People need to work to,
I think, feel productive. [02:23:11] We don't wanna offload all
of the work to the robots [02:23:13] 'cause I'm not sure if people would know [02:23:15] what to do with themselves.
(Lex laughs) [02:23:16] And I think just self-satisfaction
and feeling productive is [02:23:20] such an ingrained part of being human [02:23:23] that we need to keep doing this work. [02:23:25] So, we're definitely gonna have to work [02:23:27] in a complimentary fashion,
and I hope that the robots [02:23:30] and the computers don't end up being able [02:23:32] to do all the creative work. [02:23:34] Right?
- Yeah. [02:23:36] - 'Cause that's the part that's, you know, [02:23:37] that's the rewarding. [02:23:38] The creative part of solving
a problem is the thing [02:23:42] that gives you that serotonin
rush that you never forget, [02:23:47] you know, (laughs) or that adrenaline rush [02:23:49] that you never forget, and
so, you know, people need [02:23:53] to be able to do that creative work [02:23:55] and just feel productive, [02:23:57] and sometimes you can feel productive [02:23:59] over fairly simple work
that's just well done, [02:24:02] you know, and that you
can see the result of. [02:24:05] So, yeah, you know, I don't know, [02:24:09] there was a cartoon, was it "Wall-E," [02:24:13] where they had this big ship, [02:24:15] and all the people were just overweight, [02:24:20] lying on their beach chairs kinda sliding [02:24:22] around on the deck of the movie [02:24:25] because they didn't do anything anymore. [02:24:26] - Yeah.
- Well, [02:24:27] we definitely don't wanna
be there, (laughs) you know. [02:24:30] We need to work in some
complimentary fashion [02:24:32] where we keep all of our
faculties and our physical health, [02:24:35] and we're doing some labor, right, [02:24:37] but in a complimentary fashion somehow.